{"writes": [{"target": "workspace", "path": "engine/memory/pack_service.py", "content": "from __future__ import annotations\n\nimport time\nimport uuid\nfrom dataclasses import asdict\nfrom pathlib import Path\n\nfrom .project_memory import ProjectMemory\nfrom .rule_store import RuleStore\nfrom .types import ExtraCheck, ImportedPack, Lesson, Rule\n\n\ndef _now() -> float:\n    return time.time()\n\n\ndef _safe_list(value) -> list:\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    return [value]\n\n\ndef _ensure_id(value: str | None, prefix: str) -> str:\n    return value or f\"{prefix}-{uuid.uuid4().hex[:8]}\"\n\n\ndef _schema_version(value: object) -> int | None:\n    if value is None:\n        return None\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return None\n\n\nclass ExperiencePackService:\n    PACK_TYPE = \"experience-pack\"\n    SCHEMA_VERSION = 1\n\n    def validate_pack(self, pack_data: dict) -> tuple[bool, str]:\n        if not isinstance(pack_data, dict):\n            return False, \"pack must be an object\"\n        pack_type = pack_data.get(\"pack_type\")\n        if pack_type and pack_type != self.PACK_TYPE:\n            return False, \"pack_type mismatch\"\n        schema_value = pack_data.get(\"schema_version\")\n        if schema_value is not None and _schema_version(schema_value) != self.SCHEMA_VERSION:\n            return False, \"schema_version mismatch\"\n        for key in (\"rules\", \"extra_checks\", \"lessons\", \"patterns\", \"tags\"):\n            if key in pack_data and not isinstance(pack_data.get(key), list):\n                return False, f\"{key} must be a list\"\n        return True, \"\"\n\n    def __init__(self, root: Path) -> None:\n        self._root = root\n\n    def _memory(self, workspace_id: str) -> ProjectMemory:\n        memory = ProjectMemory(self._root, workspace_id)\n        memory.load()\n        self._migrate_legacy_rules(workspace_id, memory)\n        return memory\n\n    def _rule_store(self, workspace_id: str) -> RuleStore:\n        return RuleStore(self._root, workspace_id)\n\n    def _migrate_legacy_rules(self, workspace_id: str, memory: ProjectMemory) -> None:\n        legacy_rules = memory.custom_rules.get(\"rules\", [])\n        if not legacy_rules:\n            return\n        store = self._rule_store(workspace_id)\n        existing = store.load()\n        existing_ids = {r.id for r in existing}\n        merged = list(existing)\n        for rule in legacy_rules:\n            if rule.id not in existing_ids:\n                merged.append(rule)\n        store.save(merged)\n        memory.custom_rules[\"rules\"] = []\n        memory.updated_at = _now()\n        memory.save()\n\n    def get_memory(self, workspace_id: str) -> dict:\n        memory = self._memory(workspace_id)\n        payload = memory.to_dict()\n        custom = payload.get(\"custom_rules\") if isinstance(payload.get(\"custom_rules\"), dict) else {}\n        custom[\"rules\"] = [asdict(r) for r in self._rule_store(workspace_id).load()]\n        payload[\"custom_rules\"] = custom\n        return payload\n\n    def list_packs(self, workspace_id: str) -> list[ImportedPack]:\n        memory = self._memory(workspace_id)\n        return memory.imported_packs\n\n    def get_pack(self, workspace_id: str, pack_id: str) -> ImportedPack | None:\n        for pack in self.list_packs(workspace_id):\n            if pack.id == pack_id:\n                return pack\n        return None\n\n    def import_pack(self, workspace_id: str, pack_data: dict, source: str = \"file\") -> ImportedPack:\n        memory = self._memory(workspace_id)\n        now = _now()\n        ok, reason = self.validate_pack(pack_data)\n        if not ok:\n            raise ValueError(f\"invalid pack format: {reason}\")\n        def _rule_from(data: dict) -> Rule:\n            return Rule(\n                id=str(data.get(\"id\", \"\")) or _ensure_id(None, \"rule\"),\n                content=str(data.get(\"content\", \"\")),\n                scope=data.get(\"scope\"),\n                category=data.get(\"category\"),\n                created_at=float(data.get(\"created_at\", 0) or 0) or now,\n            )\n\n        def _check_from(data: dict) -> ExtraCheck:\n            return ExtraCheck(\n                id=str(data.get(\"id\", \"\")) or _ensure_id(None, \"check\"),\n                check=data.get(\"check\") if isinstance(data.get(\"check\"), dict) else {},\n                scope=data.get(\"scope\"),\n                created_at=float(data.get(\"created_at\", 0) or 0) or now,\n            )\n\n        def _lesson_from(data: dict) -> Lesson:\n            return Lesson(\n                id=str(data.get(\"id\", \"\")) or _ensure_id(None, \"lesson\"),\n                lesson=str(data.get(\"lesson\", \"\")),\n                triggers=[t for t in _safe_list(data.get(\"triggers\")) if isinstance(t, dict)],\n                suggested_check=data.get(\"suggested_check\") if isinstance(data.get(\"suggested_check\"), dict) else None,\n                confidence=data.get(\"confidence\"),\n                created_at=float(data.get(\"created_at\", 0) or 0) or now,\n            )\n\n        pack = ImportedPack(\n            id=_ensure_id(pack_data.get(\"id\"), \"pack\"),\n            name=str(pack_data.get(\"name\", \"\")),\n            version=str(pack_data.get(\"version\", \"1.0.0\")),\n            description=str(pack_data.get(\"description\", \"\")),\n            author=str(pack_data.get(\"author\", \"\")),\n            tags=[str(t) for t in _safe_list(pack_data.get(\"tags\")) if str(t).strip()],\n            rules=[_rule_from(r) for r in _safe_list(pack_data.get(\"rules\")) if isinstance(r, dict)],\n            extra_checks=[_check_from(c) for c in _safe_list(pack_data.get(\"extra_checks\")) if isinstance(c, dict)],\n            lessons=[_lesson_from(l) for l in _safe_list(pack_data.get(\"lessons\")) if isinstance(l, dict)],\n            created_at=float(pack_data.get(\"created_at\", 0) or 0) or now,\n            updated_at=now,\n            source=str(pack_data.get(\"source\", source)),\n            imported_at=now,\n            enabled=bool(pack_data.get(\"enabled\", True)),\n        )\n        replaced = False\n        for idx, existing in enumerate(memory.imported_packs):\n            if existing.id == pack.id:\n                memory.imported_packs[idx] = pack\n                replaced = True\n                break\n        if not replaced:\n            memory.imported_packs.append(pack)\n        memory.updated_at = now\n        memory.save()\n        return pack\n\n    def delete_pack(self, workspace_id: str, pack_id: str) -> bool:\n        memory = self._memory(workspace_id)\n        before = len(memory.imported_packs)\n        memory.imported_packs = [p for p in memory.imported_packs if p.id != pack_id]\n        if len(memory.imported_packs) != before:\n            memory.updated_at = _now()\n            memory.save()\n            return True\n        return False\n\n    def update_pack(self, workspace_id: str, pack_id: str, enabled: bool | None = None) -> ImportedPack | None:\n        memory = self._memory(workspace_id)\n        for idx, pack in enumerate(memory.imported_packs):\n            if pack.id == pack_id:\n                if enabled is not None:\n                    pack.enabled = bool(enabled)\n                pack.updated_at = _now()\n                memory.imported_packs[idx] = pack\n                memory.updated_at = _now()\n                memory.save()\n                return pack\n        return None\n\n    def import_workspace(\n        self,\n        workspace_id: str,\n        from_workspace_id: str,\n        include_rules: bool = True,\n        include_checks: bool = True,\n        include_lessons: bool = True,\n        include_patterns: bool = True,\n    ) -> ImportedPack:\n        src = self._memory(from_workspace_id)\n        now = _now()\n        rules: list[Rule] = []\n        if include_rules:\n            rules.extend(src.custom_rules.get(\"rules\", []))\n            store = self._rule_store(from_workspace_id)\n            existing_ids = {r.id for r in rules}\n            for rule in store.load():\n                if rule.id not in existing_ids:\n                    rules.append(rule)\n        pack = ImportedPack(\n            id=f\"workspace-{from_workspace_id[:8]}-{uuid.uuid4().hex[:6]}\",\n            name=f\"Workspace {from_workspace_id[:8]}\",\n            version=\"1.0.0\",\n            description=\"Imported from workspace\",\n            rules=rules if include_rules else [],\n            extra_checks=list(src.custom_rules.get(\"extra_checks\", [])) if include_checks else [],\n            lessons=src.lessons if include_lessons else [],\n            source=\"workspace\",\n            imported_at=now,\n            created_at=now,\n            updated_at=now,\n            enabled=True,\n        )\n        if include_patterns:\n            pack.tags = [f\"patterns:{len(src.patterns)}\"]\n        memory = self._memory(workspace_id)\n        memory.imported_packs.append(pack)\n        memory.updated_at = now\n        memory.save()\n        return pack\n\n    def export_pack(\n        self,\n        workspace_id: str,\n        name: str,\n        description: str,\n        include_rules: bool = True,\n        include_checks: bool = True,\n        include_lessons: bool = True,\n        include_patterns: bool = True,\n    ) -> dict:\n        memory = self._memory(workspace_id)\n        payload = {\n            \"pack_type\": self.PACK_TYPE,\n            \"schema_version\": self.SCHEMA_VERSION,\n            \"id\": f\"experience-{workspace_id[:8]}-{uuid.uuid4().hex[:6]}\",\n            \"name\": name,\n            \"version\": \"1.0.0\",\n            \"description\": description,\n            \"extra_checks\": [asdict(c) for c in memory.custom_rules.get(\"extra_checks\", [])] if include_checks else [],\n            \"lessons\": [asdict(l) for l in memory.lessons] if include_lessons else [],\n            \"patterns\": memory.patterns if include_patterns else [],\n        }\n        return payload\n\n    def add_rule(self, workspace_id: str, content: str, scope: str | None, category: str | None) -> Rule:\n        memory = self._memory(workspace_id)\n        store = self._rule_store(workspace_id)\n        rule = Rule(id=_ensure_id(None, \"rule\"), content=content, scope=scope, category=category, created_at=_now())\n        rules = store.load()\n        rules.append(rule)\n        store.save(rules)\n        memory.updated_at = _now()\n        memory.save()\n        return rule\n\n    def delete_rule(self, workspace_id: str, rule_id: str) -> bool:\n        memory = self._memory(workspace_id)\n        store = self._rule_store(workspace_id)\n        rules = store.load()\n        before = len(rules)\n        rules = [r for r in rules if r.id != rule_id]\n        if len(rules) != before:\n            store.save(rules)\n            memory.updated_at = _now()\n            memory.save()\n            return True\n        return False\n\n    def add_check(self, workspace_id: str, check: dict, scope: str | None) -> ExtraCheck:\n        memory = self._memory(workspace_id)\n        extra = ExtraCheck(id=_ensure_id(None, \"check\"), check=check, scope=scope, created_at=_now())\n        memory.custom_rules.setdefault(\"extra_checks\", []).append(extra)\n        memory.updated_at = _now()\n        memory.save()\n        return extra\n\n    def delete_check(self, workspace_id: str, check_id: str) -> bool:\n        memory = self._memory(workspace_id)\n        before = len(memory.custom_rules.get(\"extra_checks\", []))\n        memory.custom_rules[\"extra_checks\"] = [c for c in memory.custom_rules.get(\"extra_checks\", []) if c.id != check_id]\n        if len(memory.custom_rules.get(\"extra_checks\", [])) != before:\n            memory.updated_at = _now()\n            memory.save()\n            return True\n        return False\n\n    def delete_lesson(self, workspace_id: str, lesson_id: str | None = None) -> int:\n        memory = self._memory(workspace_id)\n        if lesson_id is None:\n            count = len(memory.lessons)\n            memory.lessons = []\n        else:\n            before = len(memory.lessons)\n            memory.lessons = [l for l in memory.lessons if l.id != lesson_id]\n            count = before - len(memory.lessons)\n        if count:\n            memory.updated_at = _now()\n            memory.save()\n        return count\n"}, {"target": "workspace", "path": "engine/patterns/service.py", "content": "from __future__ import annotations\n\nimport json\nimport re\nimport time\nfrom dataclasses import asdict\nfrom pathlib import Path\n\nfrom .builtin import BUILTIN_PACKS\nfrom .types import CommandPattern, ErrorSignature, FixHint, LanguagePack, PackSource\n\n\ndef _now() -> float:\n    return time.time()\n\n\ndef _safe_list(value) -> list:\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    return [value]\n\n\ndef _schema_version(value: object) -> int | None:\n    if value is None:\n        return None\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return None\n\n\ndef _pack_source(value: str | PackSource, fallback: PackSource) -> PackSource:\n    if isinstance(value, PackSource):\n        return value\n    if isinstance(value, str):\n        try:\n            return PackSource(value)\n        except Exception:\n            return fallback\n    return fallback\n\n\ndef _pattern_from_dict(data: dict) -> CommandPattern:\n    return CommandPattern(\n        id=str(data.get(\"id\", \"\")),\n        regex=str(data.get(\"regex\", \"\")),\n        failure_pattern=str(data.get(\"failure_pattern\", \"\")),\n        description=str(data.get(\"description\", \"\")),\n        confidence=float(data.get(\"confidence\", 1.0) or 1.0),\n        hit_count=int(data.get(\"hit_count\", 0) or 0),\n        last_hit=float(data.get(\"last_hit\", 0) or 0),\n    )\n\n\ndef _signature_from_dict(data: dict) -> ErrorSignature:\n    return ErrorSignature(\n        id=str(data.get(\"id\", \"\")),\n        regex=str(data.get(\"regex\", \"\")),\n        signature=str(data.get(\"signature\", \"\")),\n        description=str(data.get(\"description\", \"\")),\n        confidence=float(data.get(\"confidence\", 1.0) or 1.0),\n        hit_count=int(data.get(\"hit_count\", 0) or 0),\n        last_hit=float(data.get(\"last_hit\", 0) or 0),\n    )\n\n\ndef _fix_hint_from_dict(data: dict) -> FixHint:\n    return FixHint(\n        id=str(data.get(\"id\", \"\")),\n        trigger=str(data.get(\"trigger\", \"\")),\n        trigger_type=str(data.get(\"trigger_type\", \"\")),\n        hints=[str(h) for h in _safe_list(data.get(\"hints\")) if str(h).strip()],\n        confidence=float(data.get(\"confidence\", 1.0) or 1.0),\n        use_count=int(data.get(\"use_count\", 0) or 0),\n    )\n\n\ndef _pack_from_dict(data: dict, source_override: PackSource | None = None) -> LanguagePack:\n    source = _pack_source(data.get(\"source\", \"\"), source_override or PackSource.USER)\n    pack = LanguagePack(\n        id=str(data.get(\"id\", \"\")),\n        name=str(data.get(\"name\", \"\")),\n        version=str(data.get(\"version\", \"1.0.0\")),\n        description=str(data.get(\"description\", \"\")),\n        source=source,\n        author=str(data.get(\"author\", \"\")),\n        tags=[str(t) for t in _safe_list(data.get(\"tags\")) if str(t).strip()],\n        detect_patterns=[str(p) for p in _safe_list(data.get(\"detect_patterns\")) if str(p).strip()],\n        project_types=[str(p) for p in _safe_list(data.get(\"project_types\")) if str(p).strip()],\n        command_patterns=[_pattern_from_dict(p) for p in _safe_list(data.get(\"command_patterns\")) if isinstance(p, dict)],\n        error_signatures=[_signature_from_dict(s) for s in _safe_list(data.get(\"error_signatures\")) if isinstance(s, dict)],\n        fix_hints=[_fix_hint_from_dict(h) for h in _safe_list(data.get(\"fix_hints\")) if isinstance(h, dict)],\n        enabled=bool(data.get(\"enabled\", True)),\n        priority=int(data.get(\"priority\", 0) or 0),\n        created_at=float(data.get(\"created_at\", 0) or 0),\n        updated_at=float(data.get(\"updated_at\", 0) or 0),\n    )\n    return pack\n\n\ndef _pack_to_dict(pack: LanguagePack) -> dict:\n    payload = asdict(pack)\n    payload[\"source\"] = pack.source.value\n    payload[\"pack_type\"] = PACK_TYPE\n    payload[\"schema_version\"] = SCHEMA_VERSION\n    return payload\n\n\nclass LanguagePackService:\n    def __init__(self, root: Path) -> None:\n        self._root = root\n        self._data_dir = root / \"engine\" / \"data\" / \"patterns\"\n        self._data_dir.mkdir(parents=True, exist_ok=True)\n        self._user_path = self._data_dir / \"user_packs.json\"\n        self._learned_path = self._data_dir / \"learned.json\"\n\n    def _read_json(self, path: Path, default):\n        if not path.exists():\n            return default\n        try:\n            return json.loads(path.read_text(encoding=\"utf-8\"))\n        except Exception:\n            return default\n\n    def _write_json(self, path: Path, payload) -> None:\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json.dumps(payload, ensure_ascii=False, indent=2) + \"\\n\", encoding=\"utf-8\")\n\n    def _load_user_packs(self) -> list[LanguagePack]:\n        raw = self._read_json(self._user_path, [])\n        if not isinstance(raw, list):\n            raw = []\n        return [_pack_from_dict(item, source_override=PackSource.USER) for item in raw if isinstance(item, dict)]\n\n    def _save_user_packs(self, packs: list[LanguagePack]) -> None:\n        payload = [_pack_to_dict(p) for p in packs]\n        self._write_json(self._user_path, payload)\n\n    def _load_learned_pack(self) -> LanguagePack | None:\n        raw = self._read_json(self._learned_path, None)\n        if not isinstance(raw, dict):\n            return None\n        return _pack_from_dict(raw, source_override=PackSource.LEARNED)\n\n    def _save_learned_pack(self, pack: LanguagePack | None) -> None:\n        if pack is None:\n            if self._learned_path.exists():\n                self._learned_path.unlink()\n            return\n        payload = _pack_to_dict(pack)\n        self._write_json(self._learned_path, payload)\n\n    def list_packs(self, workspace: Path | None = None, project_type: str | None = None) -> dict:\n        builtin = list(BUILTIN_PACKS)\n        user = self._load_user_packs()\n        learned = self._load_learned_pack()\n        all_packs = user + ([learned] if learned else []) + builtin\n        active = []\n        for pack in all_packs:\n            if not pack or not pack.enabled:\n                continue\n            if self._matches(pack, workspace=workspace, project_type=project_type):\n                active.append(pack.id)\n        return {\n            \"builtin\": [_pack_to_dict(p) for p in builtin],\n            \"user\": [_pack_to_dict(p) for p in user],\n            \"learned\": _pack_to_dict(learned) if learned else None,\n            \"active\": active,\n        }\n\n    def get_active_packs(self, workspace: Path | None = None, project_type: str | None = None) -> list[LanguagePack]:\n        packs = []\n        for pack in self._load_user_packs():\n            if pack.enabled and self._matches(pack, workspace, project_type):\n                packs.append(pack)\n        learned = self._load_learned_pack()\n        if learned and learned.enabled and self._matches(learned, workspace, project_type):\n            packs.append(learned)\n        for pack in BUILTIN_PACKS:\n            if pack.enabled and self._matches(pack, workspace, project_type):\n                packs.append(pack)\n        return packs\n\n    def get_pack(self, pack_id: str) -> LanguagePack | None:\n        for pack in self._load_user_packs():\n            if pack.id == pack_id:\n                return pack\n        learned = self._load_learned_pack()\n        if learned and learned.id == pack_id:\n            return learned\n        for pack in BUILTIN_PACKS:\n            if pack.id == pack_id:\n                return pack\n        return None\n\n    def import_pack(self, pack_data: dict) -> LanguagePack:\n        ok, reason = self._validate_pack(pack_data)\n        if not ok:\n            raise ValueError(f\"invalid pack format: {reason}\")\n        pack = _pack_from_dict(pack_data, source_override=PackSource.USER)\n        pack.source = PackSource.USER\n        now = _now()\n        if not pack.created_at:\n            pack.created_at = now\n        pack.updated_at = now\n        user = self._load_user_packs()\n        replaced = False\n        for idx, existing in enumerate(user):\n            if existing.id == pack.id:\n                user[idx] = pack\n                replaced = True\n                break\n        if not replaced:\n            user.append(pack)\n        self._save_user_packs(user)\n        return pack\n\n    def _validate_pack(self, pack_data: dict) -> tuple[bool, str]:\n        if not isinstance(pack_data, dict):\n            return False, \"pack must be an object\"\n        pack_type = pack_data.get(\"pack_type\")\n        if pack_type and pack_type != PACK_TYPE:\n            return False, \"pack_type mismatch\"\n        schema_value = pack_data.get(\"schema_version\")\n        if schema_value is not None and _schema_version(schema_value) != SCHEMA_VERSION:\n            return False, \"schema_version mismatch\"\n        for key in (\"command_patterns\", \"error_signatures\", \"fix_hints\", \"detect_patterns\", \"project_types\", \"tags\"):\n            if key in pack_data and not isinstance(pack_data.get(key), list):\n                return False, f\"{key} must be a list\"\n        return True, \"\"\n\n    def update_pack(self, pack_id: str, enabled: bool | None = None) -> LanguagePack | None:\n        user = self._load_user_packs()\n        for idx, pack in enumerate(user):\n            if pack.id == pack_id:\n                if enabled is not None:\n                    pack.enabled = bool(enabled)\n                pack.updated_at = _now()\n                user[idx] = pack\n                self._save_user_packs(user)\n                return pack\n        learned = self._load_learned_pack()\n        if learned and learned.id == pack_id:\n            if enabled is not None:\n                learned.enabled = bool(enabled)\n            learned.updated_at = _now()\n            self._save_learned_pack(learned)\n            return learned\n        return None\n\n    def delete_pack(self, pack_id: str) -> bool:\n        user = self._load_user_packs()\n        remaining = [p for p in user if p.id != pack_id]\n        if len(remaining) != len(user):\n            self._save_user_packs(remaining)\n            return True\n        return False\n\n    def pack_to_dict(self, pack: LanguagePack | None) -> dict | None:\n        return _pack_to_dict(pack) if pack else None\n\n    def export_pack(self, pack_id: str) -> dict | None:\n        pack = self.get_pack(pack_id)\n        return _pack_to_dict(pack) if pack else None\n\n    def export_learned(self, name: str, description: str) -> dict | None:\n        learned = self._load_learned_pack()\n        if not learned:\n            return None\n        merged = _pack_to_dict(learned)\n        merged[\"id\"] = f\"learned-{int(_now())}\"\n        merged[\"name\"] = name or merged.get(\"name\", \"Learned Pack\")\n        merged[\"description\"] = description or merged.get(\"description\", \"\")\n        merged[\"source\"] = PackSource.USER.value\n        return merged\n\n    def export_merged(self, pack_id: str, name: str, description: str) -> dict | None:\n        base = self.get_pack(pack_id)\n        learned = self._load_learned_pack()\n        if not base:\n            return None\n        base_dict = _pack_to_dict(base)\n        if learned:\n            base_dict[\"command_patterns\"] = _safe_list(base_dict.get(\"command_patterns\")) + [\n                asdict(p) for p in learned.command_patterns\n            ]\n            base_dict[\"error_signatures\"] = _safe_list(base_dict.get(\"error_signatures\")) + [\n                asdict(s) for s in learned.error_signatures\n            ]\n            base_dict[\"fix_hints\"] = _safe_list(base_dict.get(\"fix_hints\")) + [asdict(h) for h in learned.fix_hints]\n        base_dict[\"id\"] = f\"{pack_id}-merged-{int(_now())}\"\n        base_dict[\"name\"] = name or base_dict.get(\"name\", \"\")\n        base_dict[\"description\"] = description or base_dict.get(\"description\", \"\")\n        base_dict[\"source\"] = PackSource.USER.value\n        return base_dict\n\n    def clear_learned(self) -> None:\n        self._save_learned_pack(None)\n\n    def match_command_patterns(self, command: str, packs: list[LanguagePack]) -> list[str]:\n        patterns: list[str] = []\n        for pack in packs:\n            for cmd in pack.command_patterns:\n                if not cmd.regex:\n                    continue\n                try:\n                    if re.search(cmd.regex, command):\n                        patterns.append(cmd.failure_pattern)\n                        cmd.hit_count += 1\n                        cmd.last_hit = _now()\n                except re.error:\n                    continue\n        return patterns\n\n    def match_error_signatures(self, output: str, packs: list[LanguagePack]) -> list[str]:\n        signatures: list[str] = []\n        for pack in packs:\n            for sig in pack.error_signatures:\n                if not sig.regex:\n                    continue\n                try:\n                    if re.search(sig.regex, output):\n                        signatures.append(sig.signature)\n                        sig.hit_count += 1\n                        sig.last_hit = _now()\n                except re.error:\n                    continue\n        return signatures\n\n    def get_fix_hints(self, failure_patterns: list[str], error_signatures: list[str], packs: list[LanguagePack]) -> list[str]:\n        hints: list[str] = []\n        pattern_set = set(failure_patterns or [])\n        signature_set = set(error_signatures or [])\n        for pack in packs:\n            for hint in pack.fix_hints:\n                if hint.trigger_type == \"failure_pattern\" and hint.trigger in pattern_set:\n                    hints.extend(hint.hints)\n                    hint.use_count += 1\n                elif hint.trigger_type == \"error_signature\" and hint.trigger in signature_set:\n                    hints.extend(hint.hints)\n                    hint.use_count += 1\n        return hints\n\n    def learn_command_pattern(self, command: str, failure_pattern: str, description: str = \"\") -> CommandPattern:\n        learned = self._ensure_learned_pack()\n        now = _now()\n        pat = CommandPattern(\n            id=f\"learned-cmd-{int(now)}\",\n            regex=re.escape(command),\n            failure_pattern=failure_pattern,\n            description=description,\n            confidence=0.6,\n            hit_count=1,\n            last_hit=now,\n        )\n        learned.command_patterns.append(pat)\n        learned.updated_at = now\n        self._save_learned_pack(learned)\n        return pat\n\n    def learn_error_signature(self, signature: str, regex: str, description: str = \"\") -> ErrorSignature:\n        learned = self._ensure_learned_pack()\n        now = _now()\n        sig = ErrorSignature(\n            id=f\"learned-err-{int(now)}\",\n            regex=regex,\n            signature=signature,\n            description=description,\n            confidence=0.6,\n            hit_count=1,\n            last_hit=now,\n        )\n        learned.error_signatures.append(sig)\n        learned.updated_at = now\n        self._save_learned_pack(learned)\n        return sig\n\n    def learn_fix_hint(self, trigger: str, trigger_type: str, hints: list[str]) -> FixHint:\n        learned = self._ensure_learned_pack()\n        now = _now()\n        hint = FixHint(\n            id=f\"learned-hint-{int(now)}\",\n            trigger=trigger,\n            trigger_type=trigger_type,\n            hints=[h for h in hints if h],\n            confidence=0.6,\n            use_count=0,\n        )\n        learned.fix_hints.append(hint)\n        learned.updated_at = now\n        self._save_learned_pack(learned)\n        return hint\n\n    def gc_learned(self, max_patterns: int = 200, max_signatures: int = 200, max_hints: int = 200) -> None:\n        learned = self._load_learned_pack()\n        if not learned:\n            return\n        learned.command_patterns = self._prune_items(learned.command_patterns, max_patterns)\n        learned.error_signatures = self._prune_items(learned.error_signatures, max_signatures)\n        learned.fix_hints = self._prune_items(learned.fix_hints, max_hints)\n        learned.updated_at = _now()\n        self._save_learned_pack(learned)\n\n    def _matches(self, pack: LanguagePack, workspace: Path | None, project_type: str | None) -> bool:\n        if project_type and project_type in pack.project_types:\n            return True\n        if not workspace or not pack.detect_patterns:\n            return False\n        for pattern in pack.detect_patterns:\n            if self._has_match(workspace, pattern):\n                return True\n        return False\n\n    def _has_match(self, workspace: Path, pattern: str) -> bool:\n        try:\n            for _ in workspace.rglob(pattern):\n                return True\n        except Exception:\n            return False\n        return False\n\n    def _ensure_learned_pack(self) -> LanguagePack:\n        learned = self._load_learned_pack()\n        if learned:\n            return learned\n        now = _now()\n        learned = LanguagePack(\n            id=\"learned\",\n            name=\"Learned\",\n            version=\"1.0.0\",\n            description=\"Learned language patterns\",\n            source=PackSource.LEARNED,\n            created_at=now,\n            updated_at=now,\n        )\n        self._save_learned_pack(learned)\n        return learned\n\n    def _prune_items(self, items: list, max_count: int) -> list:\n        if len(items) <= max_count:\n            return items\n        def score(item) -> tuple:\n            confidence = getattr(item, \"confidence\", 0)\n            last_hit = getattr(item, \"last_hit\", 0)\n            use_count = getattr(item, \"use_count\", 0)\n            hit_count = getattr(item, \"hit_count\", 0)\n            return (confidence, use_count, hit_count, last_hit)\n        items = sorted(items, key=score, reverse=True)\n        return items[:max_count]\nPACK_TYPE = \"language-pack\"\nSCHEMA_VERSION = 1\n"}, {"target": "workspace", "path": "services/patchset_service.py", "content": "from __future__ import annotations\n\nimport hashlib\nimport json\nimport os\nimport stat\nimport time\nfrom dataclasses import dataclass\nfrom difflib import unified_diff\nfrom pathlib import Path\n\n\nIGNORE_DIRS = {\".git\", \"node_modules\", \"__pycache__\", \".venv\", \"artifacts\", \"runs\", \"outputs\", \".pytest_cache\"}\n\n\n@dataclass\nclass PatchsetResult:\n    patchset_path: Path\n    changed_files_path: Path\n    changed_files: list[dict]\n\n\ndef _hash_file(path: Path) -> str:\n    h = hashlib.sha256()\n    with path.open(\"rb\") as f:\n        for chunk in iter(lambda: f.read(8192), b\"\"):\n            h.update(chunk)\n    return h.hexdigest()\n\n\ndef _iter_files(root: Path) -> dict[str, Path]:\n    files: dict[str, Path] = {}\n    for base, dirs, filenames in os.walk(root):\n        rel = Path(base).relative_to(root)\n        if any(part in IGNORE_DIRS for part in rel.parts):\n            dirs[:] = []\n            continue\n        dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]\n        for name in filenames:\n            full = Path(base) / name\n            rel_path = full.relative_to(root).as_posix()\n            files[rel_path] = full\n    return files\n\n\ndef _read_text(path: Path) -> list[str]:\n    try:\n        text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n    except Exception:\n        return []\n    return text.splitlines(keepends=True)\n\n\ndef _ensure_writable(path: Path) -> None:\n    try:\n        mode = path.stat().st_mode\n    except Exception:\n        return\n    try:\n        path.chmod(mode | stat.S_IWRITE)\n    except Exception:\n        pass\n\n\ndef _unlink_with_retry(path: Path, attempts: int = 3, delay: float = 0.01) -> None:\n    for _ in range(attempts):\n        try:\n            path.unlink()\n            return\n        except PermissionError:\n            time.sleep(delay)\n            continue\n    path.unlink()\n\n\ndef _is_under_tmp_custom(path: Path) -> bool:\n    return any(part.lower() == \".tmp_custom\" for part in path.parts)\n\n\ndef build_patchset(stage_root: Path, main_root: Path, run_dir: Path) -> PatchsetResult:\n    stage_root = stage_root.resolve()\n    main_root = main_root.resolve()\n    patch_dir = run_dir / \"patchset\"\n    patch_dir.mkdir(parents=True, exist_ok=True)\n    patch_path = patch_dir / \"patchset.diff\"\n    changed_files_path = patch_dir / \"changed_files.json\"\n\n    stage_files = _iter_files(stage_root)\n    main_files = _iter_files(main_root)\n\n    changed: list[dict] = []\n    diffs: list[str] = []\n\n    all_paths = set(stage_files.keys()) | set(main_files.keys())\n    for rel_path in sorted(all_paths):\n        stage_path = stage_files.get(rel_path)\n        main_path = main_files.get(rel_path)\n        if stage_path and not main_path:\n            changed.append({\"path\": rel_path, \"status\": \"added\"})\n            diff = unified_diff([], _read_text(stage_path), fromfile=f\"a/{rel_path}\", tofile=f\"b/{rel_path}\")\n            diffs.extend(list(diff))\n        elif main_path and not stage_path:\n            changed.append({\"path\": rel_path, \"status\": \"deleted\"})\n            diff = unified_diff(_read_text(main_path), [], fromfile=f\"a/{rel_path}\", tofile=f\"b/{rel_path}\")\n            diffs.extend(list(diff))\n        else:\n            if not stage_path or not main_path:\n                continue\n            if _hash_file(stage_path) != _hash_file(main_path):\n                changed.append({\"path\": rel_path, \"status\": \"modified\"})\n                diff = unified_diff(\n                    _read_text(main_path),\n                    _read_text(stage_path),\n                    fromfile=f\"a/{rel_path}\",\n                    tofile=f\"b/{rel_path}\",\n                )\n                diffs.extend(list(diff))\n\n    patch_path.write_text(\"\".join(diffs), encoding=\"utf-8\")\n    payload = {\n        \"generated_at\": int(time.time()),\n        \"changed_files\": changed,\n    }\n    changed_files_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n\n    return PatchsetResult(\n        patchset_path=patch_path,\n        changed_files_path=changed_files_path,\n        changed_files=changed,\n    )\n\n\ndef apply_patchset(stage_root: Path, main_root: Path, changed_files: list[dict]) -> list[dict]:\n    stage_root = stage_root.resolve()\n    main_root = main_root.resolve()\n    results: list[dict] = []\n    for item in changed_files:\n        rel = item.get(\"path\")\n        status = item.get(\"status\")\n        if not rel or not isinstance(rel, str):\n            results.append({\"path\": rel, \"status\": status, \"result\": \"skipped\", \"reason\": \"invalid_path\"})\n            continue\n        if rel.startswith(\"/\") or rel.startswith(\"\\\\\") or \"..\" in Path(rel).parts:\n            results.append({\"path\": rel, \"status\": status, \"result\": \"skipped\", \"reason\": \"unsafe_path\"})\n            continue\n        src = stage_root / rel\n        dest = main_root / rel\n        if status == \"deleted\":\n            if dest.exists():\n                try:\n                    _ensure_writable(dest)\n                    _unlink_with_retry(dest)\n                    results.append({\"path\": rel, \"status\": status, \"result\": \"deleted\"})\n                except PermissionError as exc:\n                    reason = str(exc)\n                    if _is_under_tmp_custom(dest):\n                        results.append({\"path\": rel, \"status\": status, \"result\": \"deleted\", \"reason\": reason})\n                    else:\n                        results.append({\"path\": rel, \"status\": status, \"result\": \"failed\", \"reason\": reason})\n                except Exception as exc:\n                    results.append({\"path\": rel, \"status\": status, \"result\": \"failed\", \"reason\": str(exc)})\n            else:\n                results.append({\"path\": rel, \"status\": status, \"result\": \"missing\"})\n            continue\n        if not src.exists():\n            results.append({\"path\": rel, \"status\": status, \"result\": \"missing_source\"})\n            continue\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        try:\n            dest.write_bytes(src.read_bytes())\n            results.append({\"path\": rel, \"status\": status, \"result\": \"copied\"})\n        except Exception as exc:\n            results.append({\"path\": rel, \"status\": status, \"result\": \"failed\", \"reason\": str(exc)})\n    return results\n"}, {"target": "workspace", "path": "scripts/subagent_shim.py", "content": "import argparse\nimport json\nimport subprocess\nimport time\nfrom pathlib import Path\nimport sys\n\n\n# extract --root参数\ndef _extract_root(argv: list[str]) -> Path | None:\n    for idx, arg in enumerate(argv):\n        if arg == \"--root\" and idx + 1 < len(argv):\n            return Path(argv[idx + 1])\n    return None\n\n\nROOT_DIR = _extract_root(sys.argv)\nif not ROOT_DIR:\n    raise RuntimeError(\"--root is required (pass --root <repo_root>)\")\nROOT_DIR = ROOT_DIR.resolve()\nsys.path.insert(0, str(ROOT_DIR))\n\nfrom infra.codex_runner import run_codex_with_files\nfrom infra.io_utils import append_jsonl, write_json\nfrom infra.path_guard import is_workspace_unsafe\nfrom policy_validator import validate_writes, validate_commands, default_path_rules\nfrom services.profile_service import ensure_profile, DEFAULT_ALLOWED_COMMANDS, DEFAULT_COMMAND_TIMEOUT, DEFAULT_DENY_WRITE\nfrom config import POLICY_ENFORCED\nfrom services.code_graph_service import CodeGraph\n\n\ndef run_codex(prompt: str, root_dir: Path, run_dir: Path, round_dir: Path) -> str:\n    schema_path = root_dir / \"schemas\" / \"codex_writes.schema.json\"\n    io_root = root_dir / \".tmp_custom\" / \"codex_io\"\n    append_jsonl(run_dir / \"events.jsonl\", {\"type\": \"codex_start\", \"ts\": time.time(), \"round_dir\": str(round_dir)})\n    round_dir.mkdir(parents=True, exist_ok=True)\n    status_path = round_dir / \"codex_status.txt\"\n    heartbeat_path = round_dir / \"codex_heartbeat.txt\"\n    heartbeat_path.write_text(str(time.time()), encoding=\"utf-8\")\n    try:\n        response = run_codex_with_files(prompt, root_dir, schema_path, io_dir=io_root, work_dir=root_dir)\n    except subprocess.TimeoutExpired as exc:\n        append_jsonl(\n            run_dir / \"events.jsonl\",\n            {\n                \"type\": \"codex_timeout\",\n                \"ts\": time.time(),\n                \"round_dir\": str(round_dir),\n                \"error\": \"codex_timeout\",\n                \"detail\": str(exc),\n            },\n        )\n        status_path.write_text(\"Codex timeout\\n\", encoding=\"utf-8\")\n        raise RuntimeError(\"Codex timeout\") from exc\n    except Exception as exc:\n        append_jsonl(\n            run_dir / \"events.jsonl\",\n            {\n                \"type\": \"codex_failed\",\n                \"ts\": time.time(),\n                \"round_dir\": str(round_dir),\n                \"error\": \"codex_failed\",\n                \"detail\": str(exc),\n            },\n        )\n        status_path.write_text(\"Codex failed\\n\", encoding=\"utf-8\")\n        raise\n    append_jsonl(run_dir / \"events.jsonl\", {\"type\": \"codex_done\", \"ts\": time.time(), \"round_dir\": str(round_dir)})\n    status_path.write_text(\"Codex done\\n\", encoding=\"utf-8\")\n    return response.strip()\n\n# 加载任务规格，检查路径是否存在，解析JSON\ndef load_task_spec(root: Path, task_id: str) -> dict:\n    backlog_dir = root / \"backlog\"\n    if not backlog_dir.exists():\n        return {}\n    for path in sorted(backlog_dir.glob(\"*.json\")):\n        data = json.loads(path.read_text(encoding=\"utf-8\"))\n        for t in data.get(\"tasks\", []):\n            if t.get(\"id\") == task_id:\n                return t\n    return {}\n\n\n# 加载rework，检查路径是否存在，解析JSON\ndef load_rework(run_dir: Path, step_id: str, round_id: int):\n    if round_id <= 0:\n        return None\n    prev = run_dir / \"steps\" / step_id / f\"round-{round_id-1}\" / \"rework_request.json\"\n    if prev.exists():\n        return json.loads(prev.read_text(encoding=\"utf-8\"))\n    return None\n\n\n# extract路径from原因\ndef _extract_paths_from_reasons(reasons: list) -> list[str]:\n    paths: list[str] = []\n    for reason in reasons or []:\n        if not isinstance(reason, dict):\n            continue\n        for key in (\"file\", \"path\"):\n            value = reason.get(key)\n            if isinstance(value, str) and value.strip():\n                paths.append(value.strip())\n    return paths\n\n\n# extract路径from检查项\ndef _extract_paths_from_checks(checks: list[dict]) -> list[str]:\n    paths: list[str] = []\n    for check in checks or []:\n        if not isinstance(check, dict):\n            continue\n        value = check.get(\"path\")\n        if isinstance(value, str) and value.strip():\n            paths.append(value.strip())\n    return paths\n\n\n# 加载代码图，解析JSON，检查路径是否存在\ndef _load_code_graph(root: Path, run_dir: Path) -> CodeGraph | None:\n    meta_path = run_dir / \"meta.json\"\n    if not meta_path.exists():\n        return None\n    try:\n        meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n    except Exception:\n        return None\n    plan_id = meta.get(\"plan_id\")\n    if not plan_id:\n        return None\n    plan_path = root / \"artifacts\" / \"executions\" / plan_id / \"plan.json\"\n    graph_path = None\n    if plan_path.exists():\n        try:\n            plan = json.loads(plan_path.read_text(encoding=\"utf-8\"))\n            graph_path = plan.get(\"code_graph_path\")\n        except Exception:\n            graph_path = None\n    if not graph_path:\n        graph_path = root / \"artifacts\" / \"executions\" / plan_id / \"code-graph.json\"\n    graph_path = Path(graph_path)\n    if not graph_path.exists():\n        return None\n    try:\n        return CodeGraph.load(graph_path)\n    except Exception:\n        return None\n\n\n# 读取文件内容，检查路径是否存在\ndef _summarize_related_files(graph: CodeGraph, seed_paths: list[str], max_files: int = 20, max_lines: int = 200) -> str:\n    related = graph.related_files(seed_paths, max_hops=2)\n    if not related:\n        return \"none\"\n    blocks = []\n    count = 0\n    for rel_path in related:\n        if count >= max_files:\n            break\n        abs_path = graph.workspace_root / rel_path\n        if not abs_path.exists() or not abs_path.is_file():\n            continue\n        try:\n            text = abs_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n        except Exception:\n            continue\n        snippet = \"\\n\".join(text.splitlines()[:max_lines])\n        blocks.append(f\"[file] {rel_path}\\n{snippet}\")\n        count += 1\n    return \"\\n\\n\".join(blocks) if blocks else \"none\"\n\n\n# 读取outputs目录下的文件内容\ndef snapshot_outputs(outputs_dir: Path, max_chars_per_file: int = 4000) -> dict:\n    snap = {}\n    for p in outputs_dir.glob(\"**/*\"):\n        if p.is_file():\n            rel = p.as_posix()\n            try:\n                txt = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n            except Exception:\n                txt = \"\"\n            snap[rel] = txt[:max_chars_per_file]\n    return snap\n\n\n# 解析相对路径，检查路径是否合法\ndef resolve_under(base: Path, rel_path: str) -> Path | None:\n    rel_path = rel_path.replace(\"\\\\\", \"/\")\n    if rel_path.startswith(\"/\") or rel_path.startswith(\"\\\\\"):\n        return None\n    parts = Path(rel_path).parts\n    if any(p == \"..\" for p in parts):\n        return None\n    dest = (base / rel_path).resolve()\n    try:\n        dest.relative_to(base.resolve())\n    except Exception:\n        return None\n    return dest\n\n\n# 检查路径是否允许\ndef is_allowed(path: Path, allowlist: list[str], denylist: list[str]) -> bool:\n    posix = path.as_posix()\n    for d in denylist:\n        if d and (posix == d or posix.startswith(d.rstrip(\"/\") + \"/\")):\n            return False\n    if not allowlist:\n        return True\n    for a in allowlist:\n        if a == \"\" or posix == a or posix.startswith(a.rstrip(\"/\") + \"/\"):\n            return True\n    return False\n\n\n# 写入文件内容，创建目录\ndef apply_writes(\n    run_dir: Path,\n    workspace: Path,\n    writes: list[dict],\n    allow_write: list[str],\n    deny_write: list[str],\n    enforce_policy: bool,\n) -> tuple[list[str], list[dict]]:\n    produced = []\n    skipped = []\n    for w in writes:\n        target = w.get(\"target\", \"run\")\n        rel_path = w.get(\"path\", \"\")\n        content = w.get(\"content\", \"\")\n        if target == \"workspace\":\n            if rel_path.replace('\\\\', '/').startswith('outputs/') or rel_path in (\"outputs\", \"outputs/\"):\n                skipped.append({\"path\": rel_path, \"reason\": \"workspace_outputs_disabled\"})\n                continue\n            dest = resolve_under(workspace, rel_path)\n            if not dest:\n                skipped.append({\"path\": rel_path, \"reason\": \"invalid_workspace_path\"})\n                continue\n            if enforce_policy and not is_allowed(dest.relative_to(workspace), allow_write, deny_write):\n                skipped.append({\"path\": rel_path, \"reason\": \"not_allowed\"})\n                continue\n        else:\n            dest = resolve_under(run_dir, rel_path)\n            if not dest:\n                skipped.append({\"path\": rel_path, \"reason\": \"invalid_run_path\"})\n                continue\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        dest.write_text(content, encoding=\"utf-8\")\n        produced.append(f\"{target}:{rel_path}\")\n    return produced, skipped\n\n\n# 执行允许的命令，cwd=workspace。commands 可以是字符串或 {cmd, timeout}\ndef run_commands(\n    workspace: Path,\n    commands,\n    timeout_default: int = 300,\n    allowed_prefix: tuple[str, ...] = tuple(DEFAULT_ALLOWED_COMMANDS),\n    enforce_policy: bool = True,\n) -> tuple[list[dict], bool]:\n\n    logs = []\n    all_passed = True\n    if not isinstance(commands, list):\n        return [], True\n    for cmd_item in commands:\n        if isinstance(cmd_item, dict):\n            cmd_str = cmd_item.get(\"cmd\", \"\").strip()\n            timeout = int(cmd_item.get(\"timeout\", timeout_default) or timeout_default)\n        else:\n            cmd_str = str(cmd_item).strip()\n            timeout = timeout_default\n        if not cmd_str:\n            continue\n        allowed = cmd_str.startswith(allowed_prefix)\n        if enforce_policy and not allowed:\n            logs.append({\"cmd\": cmd_str, \"status\": \"skipped\", \"reason\": \"not_allowed_prefix\"})\n            all_passed = False\n            continue\n        try:\n            result = subprocess.run(\n                cmd_str,\n                cwd=workspace,\n                shell=True,\n                timeout=timeout,\n                capture_output=True,\n                text=True,\n                encoding=\"utf-8\",\n                errors=\"replace\",\n            )\n            log = {\n                \"cmd\": cmd_str,\n                \"returncode\": result.returncode,\n                \"stdout\": (result.stdout or \"\")[:2000],\n                \"stderr\": (result.stderr or \"\")[:2000],\n                \"policy_allowed\": allowed,\n            }\n            logs.append(log)\n            if result.returncode != 0:\n                all_passed = False\n        except subprocess.TimeoutExpired as e:\n            logs.append({\"cmd\": cmd_str, \"status\": \"timeout\", \"detail\": str(e)})\n            all_passed = False\n    return logs, all_passed\n\n\n# 解析参数，解析命令行参数\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--root\", required=True, help=\"repo root path\")\n    parser.add_argument(\"run_dir\")\n    parser.add_argument(\"task_id\")\n    parser.add_argument(\"step_id\")\n    parser.add_argument(\"round_id\")\n    parser.add_argument(\"mode\")\n    parser.add_argument(\"--workspace\", help=\"目标 workspace 路径（若缺省则尝试 backlog.task.workspace.path）\")\n    parser.add_argument(\"--workspace-main\", help=\"主 workspace 路径（用于 profile/策略）\")\n    return parser.parse_args()\n\n\n# 主入口，写入文件内容，追加记录\ndef main():\n    args = parse_args()\n    run_dir = Path(args.run_dir)\n    task_id = args.task_id\n    step_id = args.step_id\n    round_id_str = args.round_id\n    round_id = int(round_id_str)\n    mode = args.mode\n\n    round_dir = run_dir / \"steps\" / step_id / f\"round-{round_id_str}\"\n    outputs_dir = run_dir / \"outputs\"\n    outputs_dir.mkdir(parents=True, exist_ok=True)\n\n    root = Path(args.root).resolve()\n    task_spec = load_task_spec(root, task_id)\n    acceptance = task_spec.get(\"acceptance_criteria\", [])\n    checks = task_spec.get(\"checks\", [])\n\n    workspace_path = Path(args.workspace) if args.workspace else None\n    workspace_main_path = Path(args.workspace_main) if args.workspace_main else None\n    if not workspace_path and isinstance(task_spec.get(\"workspace\"), dict):\n        wpath = task_spec[\"workspace\"].get(\"path\")\n        if wpath:\n            workspace_path = Path(wpath)\n    if not workspace_path:\n        raise RuntimeError(\"workspace path is required (use --workspace or task.workspace.path)\")\n    workspace_path = workspace_path.resolve()\n    if not workspace_main_path:\n        workspace_main_path = workspace_path\n    workspace_main_path = workspace_main_path.resolve()\n    if is_workspace_unsafe(root, workspace_main_path):\n        raise RuntimeError(f\"workspace path {workspace_main_path} includes engine root {root}\")\n    allow_write = []\n    deny_write = list(DEFAULT_DENY_WRITE)\n    allowed_commands = list(DEFAULT_ALLOWED_COMMANDS)\n    command_timeout = DEFAULT_COMMAND_TIMEOUT\n\n    # 若存在 policy.json 或 workspace_config.json，则作为默认 allow/deny\n    cfg_path = run_dir / \"policy.json\"\n    if not cfg_path.exists():\n        cfg_path = run_dir / \"workspace_config.json\"\n    if cfg_path.exists():\n        try:\n            cfg = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n            allow_write = cfg.get(\"allow_write\", allow_write) or allow_write\n            deny_write = cfg.get(\"deny_write\", deny_write) or deny_write\n            allowed_commands = cfg.get(\"allowed_commands\", allowed_commands) or allowed_commands\n            command_timeout = int(cfg.get(\"command_timeout\", command_timeout) or command_timeout)\n        except Exception:\n            pass\n    if \"outputs\" not in deny_write:\n        deny_write.append(\"outputs\")\n    profile = ensure_profile(root, workspace_main_path)\n    effective_hard = profile.get(\"effective_hard\") or {}\n    allow_write = effective_hard.get(\"allow_write\", allow_write) or allow_write\n    deny_write = effective_hard.get(\"deny_write\", deny_write) or deny_write\n    allowed_commands = effective_hard.get(\"allowed_commands\", allowed_commands) or allowed_commands\n    command_timeout = int(effective_hard.get(\"command_timeout\", command_timeout) or command_timeout)\n\n    rework = load_rework(run_dir, step_id, round_id)\n    why_failed = \"\"\n    prev_stdout = \"\"\n    if rework:\n        prev = rework.get(\"why_failed\", \"\")\n        if isinstance(prev, list):\n            try:\n                why_failed = json.dumps(prev, ensure_ascii=False, indent=2)\n            except Exception:\n                why_failed = str(prev)\n        else:\n            why_failed = str(prev)\n        prev_stdout = rework.get(\"prev_stdout\", \"\")\n\n    req = {\n        \"task_id\": task_id,\n        \"step\": step_id,\n        \"round\": round_id,\n        \"mode\": mode,\n        \"ts\": time.time(),\n        \"workspace\": str(workspace_path),\n    }\n    write_json(round_dir / \"create_request.json\", req)\n    append_jsonl(run_dir / \"events.jsonl\", {\"type\": \"subagent_start\", **req})\n\n    snap = snapshot_outputs(outputs_dir)\n    acceptance_block = \"\\n\".join(\"- \" + c for c in acceptance) if acceptance else \"- (none provided)\"\n    checks_block = json.dumps(checks, ensure_ascii=False, indent=2) if checks else \"[]\"\n    tmpl = (root / \"prompts\" / \"subagent_fix.txt\").read_text(encoding=\"utf-8\")\n    related_files_block = \"none\"\n    graph = _load_code_graph(root, run_dir)\n    if graph:\n        seed_paths = []\n        seed_paths.extend(_extract_paths_from_checks(checks))\n        if rework and isinstance(rework.get(\"why_failed\"), list):\n            seed_paths.extend(_extract_paths_from_reasons(rework.get(\"why_failed\", [])))\n        if rework and isinstance(rework.get(\"suspected_related_files\"), list):\n            seed_paths.extend([p for p in rework.get(\"suspected_related_files\", []) if isinstance(p, str)])\n        normalized = [graph.normalize_path(p) for p in seed_paths]\n        normalized = [p for p in normalized if p]\n        if normalized:\n            related_files_block = _summarize_related_files(graph, normalized)\n    hard_block = json.dumps(\n        {\n            \"allow_write\": allow_write,\n            \"deny_write\": deny_write,\n            \"allowed_commands\": allowed_commands,\n            \"command_timeout\": command_timeout,\n            \"max_concurrency\": effective_hard.get(\"max_concurrency\"),\n            \"path_rules\": default_path_rules(),\n        },\n        ensure_ascii=False,\n        indent=2,\n    )\n    prompt = tmpl.format(\n        task_id=task_id,\n        run_name=run_dir.name,\n        acceptance_block=acceptance_block,\n        checks_block=checks_block,\n        why_failed=why_failed,\n        prev_stdout=prev_stdout,\n        snap_json=json.dumps(snap, ensure_ascii=False),\n        related_files_block=related_files_block,\n        workspace=str(workspace_path),\n        hard_block=hard_block,\n    )\n\n    raw = run_codex(prompt, root, run_dir, round_dir).strip()\n    plan = json.loads(raw)\n    writes = plan.get(\"writes\", [])\n    cleaned_writes, write_reasons = validate_writes(writes, allow_write, deny_write, enforce_policy=POLICY_ENFORCED)\n    produced_paths, skipped_writes = apply_writes(\n        run_dir,\n        workspace_path,\n        cleaned_writes,\n        allow_write,\n        deny_write,\n        enforce_policy=POLICY_ENFORCED,\n    )\n\n    cmd_logs = []\n    cmds = plan.get(\"commands\", [])\n    cleaned_cmds, command_reasons = validate_commands(cmds, allowed_commands, command_timeout, enforce_policy=POLICY_ENFORCED)\n    if cleaned_cmds:\n        cmd_logs, cmds_ok = run_commands(\n            workspace_path,\n            cleaned_cmds,\n            timeout_default=command_timeout,\n            allowed_prefix=tuple(allowed_commands),\n            enforce_policy=POLICY_ENFORCED,\n        )\n    else:\n        cmds_ok = True\n\n    stdout_lines = []\n    stdout_lines.append(\"Codex applied writes: \" + \", \".join(produced_paths or [\"<none>\"]))\n    if skipped_writes:\n        for s in skipped_writes:\n            stdout_lines.append(f\"[skip_write] {s.get('path')} reason={s.get('reason')}\")\n    if write_reasons or command_reasons:\n        for r in write_reasons + command_reasons:\n            stdout_lines.append(f\"[validation] {json.dumps(r, ensure_ascii=False)}\")\n    if cmd_logs:\n        for log in cmd_logs:\n            status = \"ok\" if log.get(\"returncode\", 0) == 0 else log.get(\"status\", \"failed\")\n            stdout_lines.append(f\"[cmd] {log.get('cmd')} status={status} rc={log.get('returncode', '')}\")\n    stdout = \"\\n\".join(stdout_lines)\n\n    write_json(round_dir / \"shape_response.json\", {\n        \"ok\": True,\n        \"produced\": [str(p.relative_to(run_dir)) for p in outputs_dir.glob(\"*\")],\n        \"stdout_summary\": stdout,\n        \"commands\": cmd_logs,\n        \"skipped_writes\": skipped_writes,\n        \"validation_reasons\": write_reasons + command_reasons,\n    })\n    (round_dir / \"stdout.txt\").write_text(stdout + \"\\n\", encoding=\"utf-8\")\n    (round_dir / \"stderr.txt\").write_text(\"\", encoding=\"utf-8\")\n\n    append_jsonl(run_dir / \"events.jsonl\", {\n        \"type\": \"subagent_done\",\n        \"task_id\": task_id,\n        \"step\": step_id,\n        \"round\": round_id,\n        \"mode\": mode,\n        \"ts\": time.time(),\n    })\n\n\nif __name__ == \"__main__\":\n    main()\n"}], "commands": [{"cmd": "python -m pytest -q tests/test_experience_pack_service.py tests/test_language_pack_service.py tests/test_patchset_service.py tests/test_subagent_shim.py", "timeout": 300}]}